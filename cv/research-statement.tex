\documentclass[10pt]{article}
\usepackage{charter}
\usepackage{textcomp}
\usepackage{stmaryrd}
\usepackage{fullpage}
\usepackage[usenames,dvipsnames,svgnames]{xcolor}
\usepackage{hyperref}
\usepackage[flushleft,neverdecrease,neveradjust]{paralist}
\pagestyle{empty}
\raggedbottom
\raggedright

\textheight=9.0in
\setlength{\tabcolsep}{0in}
\setlength{\oddsidemargin}{-0.5cm}
\setlength{\evensidemargin}{-0.5cm}
\setlength{\textwidth}{7.0in}
\usepackage[T1]{fontenc}

\renewcommand{\labelitemi}{}
\renewcommand{\labelitemii}{}

\hypersetup{
  colorlinks=true,
  urlcolor=Sepia,
  pdfborder= 0 0 0,
  bookmarks=false,
  pdftitle={Siddharth Narayanaswamy - Research Statement},
  pdfauthor={Siddharth Narayanaswamy},
  pdfsubject={Research Statement}}

\newenvironment{researchBlock}[1]{%
  \vspace*{0.5ex}
  {\large \textbf{#1}}
  \begin{enumerate}\item[]}
  {\end{enumerate}}

\newcommand{\refr}[1]{{\color{RoyalBlue} #1}}
\newcommand{\LincolnLogs}{\textsc{Lincoln Logs}}
\newcommand{\LincolnLog}{\textsc{Lincoln Log}}
\newcommand{\ie}{\emph{i.e.,}}

\defaultleftmargin{10pt}{}{}{}

\begin{document}

\begin{tabular*}{6.86in}{l@{\extracolsep{\fill}}r}
  \textbf{\huge{Research Statement}} & \today
\end{tabular*}
\vspace{0.1in}

\begin{tabular*}{6.86in}{l@{\extracolsep{\fill}}r}
  \textbf{\large{Siddharth Narayanaswamy}} & \url{http://www.iffsid.com}
\end{tabular*}
\vspace{0.4in}

\begin{researchBlock} {Reasoning about part-based 3D structures}
  %
  Many physical entities in the world exhibit a compositional structure, in
  that they are composed of different kinds of parts, arranged in certain
  fashions.
  %
  As humans, we are able to perceive the underlying structure of such entities,
  interact with them, and describe them where necessary.
  %
  In order to do so, we collate information within and across vision, language,
  and action, in the context of what we know about the physics of the world.
  %
  The fact that we are able to achieve this, is however, no indication of the
  ease of the problem; quite the opposite, in fact.
  %
  The general dearth of distinctive visual features and an overabundance of
  occlusion combine to exacerbate the difficulty of the process.

  We develop an integrated and cross-modal approach which can perceive,
  manipulate, and describe part-based 3D structures.
  %
  It combines knowledge about the different kinds of parts used to construct a
  structure and knowledge about how such parts may combine physically to form
  stable configurations, to reason about the complete part-by-part composition
  of such a structure.
  %
  In doing so, it incorporates reasoning about occlusion, in that reasoning
  about the part-by-part composition of 3D structures necessitates reasoning
  about the parts of it that cannot be seen due to occlusion.
  %
  Furthermore, it estimates a confidence measure of the underlying
  part-by-part composition perceived, mimicking the manner in which people
  evaluate the goodness of their own perception.
  %
  All of this is facilitated by representing the knowledge about the
  parts, how they combine, and some basic world knowledge in a large graphical
  model, using a probabilistic programming language.

  The compositional nature of vision, language, and actions that one can
  perform in this domain, enable the natural integration of these modalities
  into the model.
  %
  For example, if it is determined that a large part of the structure is
  occluded, a possible course of action might be to view the structure from a
  different view-point.
  %
  An alternate course of action would be to peek inside the structure.
  %
  Yet another approach could involve asking for a natural-language description
  from someone who knows more about the structure.
  %
  Not only are we able to emulate such cross-modal approaches due to the
  integrative nature of the framework, we are able to reason about \emph{which}
  action(s) to perform.
  %
  Such capability is derived from an \emph{imaginative} approach to exploration
  of the domain, evaluating which imagined view, for example, best increases
  potential confidence in perception from the current view.
  %
  We use such a framework to build structures from natural-language input,
  describe an observed structures in language, and manipulate such structures
  based on resulting percepts. We demonstrate this approach using a physical
  robots in the domain of \LincolnLogs, a children's assembly toy.

  Publications: \refr{J2}, \refr{C6}, and \refr{C3}
\end{researchBlock}

\begin{researchBlock} {Learning rules of games through perception}
  %
  Children learn to play games by watching others play.
  %
  While both formal board games such as Chess and Checkers, and less formal
  play such as Hopscotch and Tag, all have well defined rules that children
  ultimately come to know, they are rarely told those rules explicitly.
  %
  More generally, such ability to learn is crucial, as often there is no fixed
  set of codified rules that one could be given explicitly.
  %
  Inspired by children's ability to learn rules about the world from
  demonstration, instruction, and interaction, we model, in the domain of board
  games, an analogous system that learns the rules of board games from visual
  observation.
  %
  The goal here is to learn \emph{how} to play, rather than to play
  \emph{well}.

  To this end, we develop a framework that features an integrated vision and
  robotic system that plays, and learns to play, physically-instantiated board
  games, in the context of basic knowledge about the world such as spatial
  relations, ownership, and opponency.
  %
  Two robotic agents are given the rules of a game in natural language which
  drives their play. A third robotic agent, the learner, which does not know
  the rules in advance, watches the other agents play the game.
  %
  It then attempts to learn \emph{how} to play the game, \ie\ the rules of the
  game, solely from the observed input.
  %
  The learned rules are subsequently used to drive the gameplay of the learner
  against one of the original agents, to test how well it has learned the
  rules.
  %
  The physical instantiation in the form of robotic agents forces the inference
  process, driven by Inductive Logic Programming (ILP), to happen from
  real-world input.
  %
  Using this framework, we learn the rules of six games --- Tic-Tac-Toe,
  Hexapawn and 4 different variants of Hexapawn, only needing to observe a
  small number (3--5) of games.
  %
  Such a framework enables the exploration of a teacher-student relationship,
  where, for example, learning an incomplete set of rules could trigger a
  conversation with a teacher to fill in the missing gaps, or corrections may
  be issued by the teacher for rules learned incorrectly by the student.

  Publications: \refr{C2}
\end{researchBlock}

\begin{researchBlock} {Compositional representation of events in the human brain}
  %
  Humans exhibit the ability to describe what they see in natural language.
  %
  Such description often involves subjects, objects, actions and the like,
  structured in a compositional fashion, which is often exploited.
  %
  For example, in computer vision, representations of event participants,
  corresponding to nouns, are often independent of representations for the
  actions, corresponding to verbs, when dealing with the problem of activity
  recognition.
  %
  While we observe and represent these concepts in a compositional form, it is
  not clear if the brain represents these concepts in such a compositional
  fashion.

  To investigate if humans employ compositional representations, we conducted
  an experiment where subjects viewed video clips of an activity involving
  prepositions, verbs, and multiple nouns during neuroimaging (fMRI) sessions,
  and we attempted to decode the labels for the video clips from the resulting
  brain scans.
  %
  One of the key facets of this experiment was the comparison between decoding
  a complex concept and the decoding its constituents.
  %
  Prior work relating to the recovery of individual constituents focused
  primarily on the recovery of nouns, whereas, in order to conduct our
  experiment, we required the ability to decode other components such as verbs;
  something we addressed by running a separate experiment to test the
  feasibility of recovering verbs from visual stimuli.
  %
  Here, we decoded labels corresponding to one of six verbs: carry, dig, hold,
  pick up, put down, and walk from stimuli in the form of video clips that
  depicted one of those events.
  %
  We were able to successfully recover the correct verb label with an accuracy
  of 65\%, where chance is 16.66\%.
  %
  Extending this manner of experiment to the other constituents such as
  prepositions enabled us to conduct the comparison between recovering complex
  concepts and recovering its constituents.
  %
  This experiment involved showing subjects videos which depict three verbs
  (carry, fold, and leave), each performed with three objects (chair, shirt,
  and tortilla), each performed by four different human actors, and each
  performed on either side of the field of view.
  %
  If the representation of complex concepts is indeed compositional, we expect
  that the extent to which one could recover the label of a complex concept
  such as an \emph{event}, an action performed with an object, would match the
  extent to which one could recover its constituents, the action and the
  object, independently.
  %
  We expect that decoding the object noun will provide little to no information
  about the verb, and vice versa, given that our stimuli, as designed, depict
  each action performed with each object.
  %
  We find that this is indeed backed up by experimental analyses, which
  indicate that the extent to which we can recover nouns and verbs jointly
  (48\% accuracy), \ie, the \emph{event}, is comparable to the extent to which
  we can recover each independently (46\% accuracy), providing support for the
  compositionality of representations in the brain.
  %
  We show this for a variety of complex concepts.
  %
  Furthermore, we show that the brain regions associated with the individual
  concepts are largely disjoint, and the regions associated with complex
  concepts largely ovelap with those corresponding to the union of individual
  concepts, lending further evidence to the decomposability of the neural
  representations of complex concepts.

  Collaborators: Jason Corso (SUNY Buffalo), Stephen Hanson (Rutgers), Barak Pearlmutter (NUI Maynooth),
  Tom Talavage (Purdue), and Ronnie Wilbur (Purdue).

  Publications: \refr{T2}
\end{researchBlock}

\begin{researchBlock} {Activity Recognition}

  Here, we show how the compositional structure of events, in concert with the
  compositional structure of language, enables a unified framework relating to
  activity recognition in video.
  %
  This allows for a diverse range of capabilities, integrating across object
  recognition, tracking, activity recognition, and natural-language generation
  of descriptions.

  We first develop a detection-based tracker, referred to as the \emph{viterbi
    tracker}, which constructs sequences of detections produced by candidate
  object detectors that tracks an object through a video.
  %
  We structure this problem in an analogous manner to finding optimal the state
  sequence in event model lattices, which facilitates efficient tracking of
  objects in video using the Viterbi algorithm.
  %
  Such structure is further exploited to enable the integration of top-down
  information, in the form of event models, with bottom-up information, in the
  form of tracking, enabling the knowledge of an event to affect the tracks
  produced.
  %
  This is made possible by viewing the tracking and even-recognition processes
  through the common lens of finding an optimal path through a time-series
  lattice, and combining them, except that instead of optimizing the two
  processes independently, we do so jointly.
  %
  We refer to this as the \emph{event tracker}.

  We further extend the event tracker to develop the \emph{sentence tracker},
  which includes event models not just corresponding to verbs, but to other
  parts of speech as well, such as prepositions and adverbs, to enable the
  integration of top-down information in the form of an entire sentence, with
  the trackers.
  %
  This is again facilitated by the shared underlying structure of the trackers
  and event models, jointly optimizing a set of trackers, one for each
  participant in an activity, and a set of event models, one for each word in a
  given description.
  %
  At its core, the sentence tracker is a method that produces, for a given
  video and a description, a score along with a set of tracks corresponding to
  that description.
  %
  Such an integrated framework, simply by leveraging its features in different
  manners, allows for a multitude of tasks:
  %
  \begin{compactdesc}
  \item[focus of attention] Using the tracks that it produces, it can take a
    sentential description as input and focus the attention of a tracker on the
    activity described in the sentence.
    %
    In a video that depicts many participants, various subsets of whom are
    engaged is different activities, as is typical in a natural context, this
    allows us to track those particular participants that are engaged in a
    particular activity as specified by the description.
    %
  \item[generation] Using the score that it produces, it can generate
    sentential descriptions for videos by efficiently searching through the
    space of possible sentences to find one that best describes a given video.
    %
  \item[retrieval] Using the score that it produces, it can perform
    content-based video retrieval by searching through a corpus of videos to
    find that video which best depicts the activity described by a given
    description.
  \end{compactdesc}

  Collaborators: Sven Dickinson (Toronto) and Song Wang (South Carolina).

  Publications: \refr{J1}, \refr{C7}, \refr{C5}, \refr{C4}, \refr{T4}, and
  \refr{T3}
\end{researchBlock}

\end{document}

%  LocalWords:  Siddharth Narayanaswamy compositional modalities percepts ILP
%  LocalWords:  opponency gameplay instantiation Tac Hexapawn compositionality
%  LocalWords:  fMRI sentential pre scalability multi viterbi Felzenszwalb GPU
%  LocalWords:  children's neuroimaging versa Corso SUNY Barak Pearlmutter NUI
%  LocalWords:  Maynooth Talavage detections
